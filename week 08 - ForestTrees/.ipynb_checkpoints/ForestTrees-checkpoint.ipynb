{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73da965-5065-468a-a50a-a76fad46d901",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "\n",
    "Welcome! In this notebook, we'll explore two powerful and interpretable machine learning models: **Decision Trees** and **Random Forests**.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- What decision trees are and how they split data\n",
    "- Entropy, Information Gain, and Gini Index\n",
    "- Overfitting and pruning\n",
    "- Random forests and the idea of bagging\n",
    "- Implementing models with `scikit-learn`\n",
    "- Visualizing and interpreting models\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be6a66-4b7a-472f-8a2e-1daffacab0d3",
   "metadata": {},
   "source": [
    "### üå≥ Decision Trees\n",
    "---\n",
    "\n",
    "#### üìå 1. What is a Decision Tree?\n",
    "\n",
    "A **Decision Tree** is a model that makes decisions by recursively splitting data based on feature values.\n",
    "\n",
    "- **Root Node**: The first decision point.  \n",
    "- **Internal Nodes**: Intermediate decisions based on features.  \n",
    "- **Leaf Nodes**: Final output (e.g., class label or predicted value).\n",
    "\n",
    "At each step, the goal is to choose a split that best separates the data into \"pure\" subsets ‚Äî groups where most samples belong to the same class.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ 2. Splitting Criteria\n",
    "\n",
    "To build the tree, we need a way to measure how \"impure\" a group of data is, and how much a split reduces that impurity.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìä Entropy and Information Gain\n",
    "\n",
    "**Entropy** measures uncertainty or disorder in a dataset:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(S) = -\\sum_{i=1}^{C} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $S$ is the dataset at a node  \n",
    "- $C$ is the number of classes  \n",
    "- $p_i$ is the proportion of samples in class $i$\n",
    "\n",
    "**Examples**:  \n",
    "- Perfectly mixed classes (e.g., 50/50): $\\text{Entropy} = 1$  \n",
    "- Pure class (all one label): $\\text{Entropy} = 0$\n",
    "\n",
    "---\n",
    "\n",
    "**Information Gain (IG)** measures how much a feature reduces entropy:\n",
    "\n",
    "$$\n",
    "IG(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $A$ is a feature to split on  \n",
    "- $\\text{Values}(A)$ are the unique values of feature $A$  \n",
    "- $S_v$ is the subset of $S$ where feature $A = v$\n",
    "\n",
    "---\n",
    "\n",
    "##### üåÄ Gini Impurity\n",
    "\n",
    "An alternative to entropy is the **Gini Impurity**:\n",
    "\n",
    "$$\n",
    "\\text{Gini}(S) = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Gini is often used in practice (e.g., it's the default in `scikit-learn`) because it's simpler to compute and works well in many cases.\n",
    "\n",
    "---\n",
    "\n",
    "#### ü™ì 3. Splitting Process\n",
    "\n",
    "At each node:\n",
    "1. Evaluate all features (and possible thresholds for numerical features).\n",
    "2. For each, compute the entropy or Gini impurity of the resulting splits.\n",
    "3. Choose the feature and threshold that maximize Information Gain (or minimize impurity).\n",
    "4. Recursively split until a stopping condition is met (e.g., maximum depth, minimum samples per leaf, or pure leaf).\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è 4. Overfitting and Pruning\n",
    "\n",
    "Decision Trees can easily overfit the training data.\n",
    "\n",
    "**How to control overfitting:**\n",
    "- Limit the **maximum depth** of the tree\n",
    "- Set a **minimum number of samples** required to split a node\n",
    "- Use **pruning** (remove branches with low predictive power)\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ 5. Advantages and Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "- Easy to understand and interpret\n",
    "- Handles numerical and categorical data\n",
    "- No feature scaling required\n",
    "- Fast training and prediction\n",
    "\n",
    "**Disadvantages:**\n",
    "- Prone to overfitting\n",
    "- Small changes in data can lead to very different trees\n",
    "- Less accurate than ensemble methods like Random Forests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22edc0a-5cee-4835-9aec-05f278b2a6f2",
   "metadata": {},
   "source": [
    "### üåü Worked Example: Splitting with Entropy and Information Gain\n",
    "\n",
    "Let's say we have a small dataset of 10 students with the following features:\n",
    "\n",
    "| Student | Studies? | Passes Exam |\n",
    "|---------|----------|--------------|\n",
    "| 1       | Yes      | Yes          |\n",
    "| 2       | Yes      | Yes          |\n",
    "| 3       | No       | No           |\n",
    "| 4       | Yes      | Yes          |\n",
    "| 5       | No       | No           |\n",
    "| 6       | No       | No           |\n",
    "| 7       | Yes      | Yes          |\n",
    "| 8       | No       | No           |\n",
    "| 9       | Yes      | Yes          |\n",
    "| 10      | No       | No           |\n",
    "\n",
    "We want to predict whether a student **passes the exam**, and we‚Äôre considering splitting the data based on whether they **study**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Calculate Entropy of the Full Dataset\n",
    "\n",
    "There are:\n",
    "- 5 students who **pass** (Yes)\n",
    "- 5 students who **fail** (No)\n",
    "\n",
    "So the entropy of the whole dataset is:\n",
    "\n",
    "$$\n",
    "Entropy(S) = -p_+ \\log_2 p_+ - p_- \\log_2 p_-\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_+ = 5/10 = 0.5$\n",
    "- $p_- = 5/10 = 0.5$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "Entropy(S) = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Split the Data on \"Studies?\"\n",
    "\n",
    "- Students who **study (Yes)**: 5 students ‚Üí All passed\n",
    "- Students who **don‚Äôt study (No)**: 5 students ‚Üí All failed\n",
    "\n",
    "So we split into two groups:\n",
    "\n",
    "**Group 1 (Studies = Yes):**  \n",
    "- 5 students  \n",
    "- 5 pass, 0 fail ‚Üí pure group  \n",
    "- Entropy = 0\n",
    "\n",
    "**Group 2 (Studies = No):**  \n",
    "- 5 students  \n",
    "- 0 pass, 5 fail ‚Üí pure group  \n",
    "- Entropy = 0\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Calculate Weighted Average Entropy After Split\n",
    "\n",
    "$$\n",
    "Entropy_{after} = \\frac{5}{10} \\cdot 0 + \\frac{5}{10} \\cdot 0 = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Calculate Information Gain\n",
    "\n",
    "$$\n",
    "IG = Entropy(S) - Entropy_{after} = 1 - 0 = 1\n",
    "$$\n",
    "\n",
    "So, splitting on \"Studies?\" gives us **perfect information gain**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f6aa4-6775-4115-80a7-bba3d2e7a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561bcd1-4cc3-4c74-92bf-38850739d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afbc7e-f3b6-484a-9817-c52b6b52c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa7c68-e19a-4d8d-b28e-e50cac741c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Decision Tree\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76355b-0225-41fa-99d6-8e26a67be387",
   "metadata": {},
   "source": [
    "### üìè Precision, Recall, and F1-Score\n",
    "\n",
    "In a binary classification setting, we define:\n",
    "\n",
    "- **True Positives (TP)**: Model correctly predicts positive class  \n",
    "- **False Positives (FP)**: Model incorrectly predicts positive class  \n",
    "- **False Negatives (FN)**: Model misses the positive class  \n",
    "- **True Negatives (TN)**: Model correctly predicts negative class\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Precision\n",
    "\n",
    "Precision tells us **how many of the predicted positives were actually correct**.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "Recall tells us **how many of the actual positives were captured** by the model.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ F1-Score\n",
    "\n",
    "F1-score is the **harmonic mean** of precision and recall. It balances the two metrics.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "These metrics are especially useful in **imbalanced classification problems**, where accuracy can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ed25a-952d-4693-8e54-9731c3628b52",
   "metadata": {},
   "source": [
    "#### üü¶ Macro Average\n",
    "\n",
    "- **Macro avg** computes the metric **independently for each class**, and then **takes the unweighted mean**.\n",
    "- **All classes are treated equally**, regardless of how many samples each class has.\n",
    "\n",
    "**Example formula for Macro Precision:**\n",
    "\n",
    "$$\n",
    "\\text{Precision}_{macro} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{Precision}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ is the number of classes\n",
    "- $\\text{Precision}_i$ is the precision for class $i$\n",
    "\n",
    "Same idea applies to recall and F1-score.\n",
    "\n",
    "‚úÖ Use macro avg if you care equally about **all classes**, even rare ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### üü® Weighted Average\n",
    "\n",
    "- **Weighted avg** computes the metric for each class and **weights it by the number of true instances** (support) in that class.\n",
    "- Classes with more samples have more influence on the final score.\n",
    "\n",
    "**Example formula for Weighted Precision:**\n",
    "\n",
    "$$\n",
    "\\text{Precision}_{weighted} = \\sum_{i=1}^{C} \\frac{n_i}{N} \\cdot \\text{Precision}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n_i$ = number of true samples in class $i$\n",
    "- $N$ = total number of samples\n",
    "- $\\text{Precision}_i$ = precision for class $i$\n",
    "\n",
    "‚úÖ Use weighted avg when you want a score that reflects **class imbalance**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "\n",
    "| Metric Type   | Treats Classes Equally? | Accounts for Class Imbalance? |\n",
    "|---------------|-------------------------|-------------------------------|\n",
    "| Macro Average | ‚úÖ Yes                  | ‚ùå No                         |\n",
    "| Weighted Avg  | ‚ùå No                   | ‚úÖ Yes                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31487c4f-0145-445a-b4d4-98c8f170eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dffefb-6e1c-4432-adc9-4d466bda2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Use only sepal length and sepal width for 2D plot\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Train a Decision Tree\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# üß† Predict class for each point in mesh\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# üé® Plot the regions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "# Plot training points\n",
    "for i, class_name in enumerate(target_names):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], label=class_name, edgecolor='k', s=50)\n",
    "\n",
    "plt.xlabel('Sepal length (cm)')\n",
    "plt.ylabel('Sepal width (cm)')\n",
    "plt.title('Decision Tree Decision Regions (Iris Dataset)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76a3cf-1448-4bb8-ae92-4d796a4bac1c",
   "metadata": {},
   "source": [
    "### üå≤ Random Forests\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå **What is a Random Forest?**\n",
    "\n",
    "A **Random Forest** is an ensemble learning method that combines multiple **decision trees** to create a stronger and more accurate model. The key idea behind Random Forests is that by averaging many trees, the overall model becomes more robust and generalizes better than a single decision tree, which may overfit the data.\n",
    "\n",
    "- **Ensemble Learning**: This refers to combining the predictions of multiple models to improve the overall performance.\n",
    "- **Randomness**: Random Forests introduce randomness both in the selection of the training data and in the features used to build individual trees.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† **How Does a Random Forest Work?**\n",
    "\n",
    "1. **Bootstrap Sampling**:  \n",
    "   Random Forests build each tree using a different random subset of the training data. This subset is created by **bootstrap sampling** ‚Äî randomly selecting data points with replacement. As a result, each tree is trained on slightly different data.\n",
    "\n",
    "2. **Random Feature Selection**:  \n",
    "   When building each decision tree, Random Forests don't consider all features for each split. Instead, they randomly select a subset of features at each node to decide the best split. This introduces additional diversity among the trees, making the forest stronger and reducing overfitting.\n",
    "\n",
    "3. **Building Many Decision Trees**:  \n",
    "   The model trains a large number of decision trees, each on a different subset of data and features. Each tree makes independent predictions based on the data it sees.\n",
    "\n",
    "4. **Averaging or Voting**:  \n",
    "   For **regression tasks**, Random Forests average the predictions of all the trees.  \n",
    "   For **classification tasks**, Random Forests use majority voting, where the class predicted by the majority of trees becomes the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä **Key Concepts in Random Forests**\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating)**:  \n",
    "  Bagging refers to the method of training each tree on a random subset of the data. The key idea is that training on different data subsets reduces the variance of the model without increasing bias too much.\n",
    "  \n",
    "- **Out-of-Bag (OOB) Error**:  \n",
    "  For each tree, some data points are not selected during the bootstrap sampling. These points are called **out-of-bag** samples. The performance of the Random Forest can be evaluated using these OOB samples, which serve as a validation set for each tree.\n",
    "\n",
    "- **Feature Randomness**:  \n",
    "  At each node of the decision tree, only a random subset of features is considered for the best split. This ensures that the trees in the forest are diverse and reduces the risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö° **Advantages of Random Forests**\n",
    "\n",
    "1. **Robustness**:  \n",
    "   By combining many decision trees, Random Forests are less likely to overfit compared to a single decision tree. Even if some trees overfit, the random selection of data and features reduces the impact on the final model.\n",
    "\n",
    "2. **Handling Missing Values**:  \n",
    "   Random Forests are capable of handling missing values, especially if there are many trees in the forest. The algorithm can still make good predictions even when some data points are missing.\n",
    "\n",
    "3. **Feature Importance**:  \n",
    "   Random Forests can provide insights into which features are the most important in predicting the target variable. This is done by measuring how much each feature improves the split quality in the trees.\n",
    "\n",
    "4. **Versatility**:  \n",
    "   Random Forests can be used for both **classification** and **regression** tasks and are very flexible in handling different types of data, including numerical, categorical, and missing data.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è **Disadvantages of Random Forests**\n",
    "\n",
    "1. **Model Interpretability**:  \n",
    "   Random Forests are often seen as a \"black-box\" model, meaning they are difficult to interpret. Unlike a single decision tree, which is easy to visualize and understand, the random forest's decision-making process is more complex.\n",
    "\n",
    "2. **Computation Cost**:  \n",
    "   Since Random Forests require the training of many decision trees, they can be computationally expensive and require more memory than individual decision trees.\n",
    "\n",
    "3. **Slow Predictions**:  \n",
    "   In some cases, predictions can be slower than using a single decision tree, especially when the number of trees in the forest is large.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìà **Summary**\n",
    "\n",
    "- A **Random Forest** is an ensemble method built by training many decision trees on random subsets of the data and features.\n",
    "- It combines **bagging** and **random feature selection** to create a robust model that performs well on a wide range of tasks.\n",
    "- Random Forests offer strong generalization, high accuracy, and can handle missing data, but they come at the cost of model interpretability and computational resources.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cdd64-1c1b-4b3a-87c1-0bfcff3dc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176f7e0-9800-411c-9685-da37fbab377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "feature_names = wine.feature_names\n",
    "target_names = wine.target_names\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d99d1a-b28f-4e93-a837-685b14eb7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568ca74-b4d0-4e5d-b1fb-2e0be925f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dff43-0896-4740-9642-d3c991b73025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Display feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature Importances (Random Forest - Wine Dataset)\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b0462-c648-4556-a718-6575c8912366",
   "metadata": {},
   "source": [
    "$$\\text{Importance}(f) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{n \\in I(f, t)} \\Delta i_t^n\n",
    "$$\n",
    "- $T$ is the total number of trees in the forest\n",
    "- $f$ is the feature for which importance is being calculated\n",
    "- $I(f,t)$ is the set of nodes in tree $t$ where feature $f$ is used\n",
    "- $\\Delta i_T^n$ is the impurity decrease at node $n$ in tree $t$\n",
    "\n",
    "$$\\tilde{I}(f) = \\frac{\\text{Importance}(f)}{\\sum_{f'} \\text{Importance}(f')}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09396bd-ddda-4ce0-8746-11804c570826",
   "metadata": {},
   "source": [
    "## ‚ö° Boosting\n",
    "- Models are trained **sequentially**, with each trying to **fix the errors** of the previous one.\n",
    "- Later models give **more weight to misclassified** points.\n",
    "- Final prediction is a **weighted vote**.\n",
    "\n",
    "We will use `AdaBoostClassifier` (short for **Adaptative Boosting**) to illustrate this method. \n",
    "- This method works sequentially, training each new model to focus more on the mistakes made by the previous ones.\n",
    "- Each weak learner is assigned with a weight based on its performance.\n",
    "- The final prediction is a weighted majority vote of all the learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ebc2e-2478-41a6-980a-f8d2347f7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30342e24-a80b-4279-afaf-76f7f59dc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from UCI (preloaded version from seaborn or you can download it externally)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(url, names=columns, na_values=\" ?\", skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254c637-c1ac-49a7-9d1f-03c394233046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ffcc4-4a9a-4fd7-8c0a-690836f99f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78be076-21bc-40d9-aa8f-fec2a97195a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe5196-1bf8-48e3-879c-19befa7cecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe0a42-b3fe-47b2-9e8c-bf5f2c378c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = data.drop(\"income\", axis=1)\n",
    "y = data[\"income\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9ca55-c496-4e20-8618-c0d415883b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost model\n",
    "boosting_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "boosting_model.fit(X_train, y_train)\n",
    "y_pred = boosting_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoders['income'].classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
